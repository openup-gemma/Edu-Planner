# -*- coding: utf-8 -*-
"""notebookc273e718b5

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/#fileId=https%3A//storage.googleapis.com/kaggle-colab-exported-notebooks/notebookc273e718b5-6fd91b60-44c3-480c-9e85-0fc395227638.ipynb%3FX-Goog-Algorithm%3DGOOG4-RSA-SHA256%26X-Goog-Credential%3Dgcp-kaggle-com%2540kaggle-161607.iam.gserviceaccount.com/20240514/auto/storage/goog4_request%26X-Goog-Date%3D20240514T072102Z%26X-Goog-Expires%3D259200%26X-Goog-SignedHeaders%3Dhost%26X-Goog-Signature%3D27f3317640302340c85680fc7160c8bfc030a77e062018d131188ee1b442fdf275815d4ce0fad4ba69e837121a11375aebaf0e931fef7b4ec6368d73b5cc738a286430d9a4683c7821fff968ae7777eaaa6a100f67b3a3f8fa91eedee5741aedf3869f645c2ce9f9756021ae7801a9530af2f4c6879ab66451df6015d0dd910724c07f9ab28bf34ee7f6b7fcf0606e9e02d40796bbd7c9e612bf94f563da5ca57fb42496a3062225d9b4dc2670ca67f3925b19ab003dd7eb2b28c0cbfd40129c2c62471deb92bca2904041734d4c99ed5326d878c37cd9699246a9ac44e4b63b4ae6cf35d81658cd187c00a4fe665da719e8a04465bc31315ad63b4bc20aa04d
"""

# IMPORTANT: RUN THIS CELL IN ORDER TO IMPORT YOUR KAGGLE DATA SOURCES
# TO THE CORRECT LOCATION (/kaggle/input) IN YOUR NOTEBOOK,
# THEN FEEL FREE TO DELETE THIS CELL.
# NOTE: THIS NOTEBOOK ENVIRONMENT DIFFERS FROM KAGGLE'S PYTHON
# ENVIRONMENT SO THERE MAY BE MISSING LIBRARIES USED BY YOUR
# NOTEBOOK.

import os
import sys
from tempfile import NamedTemporaryFile
from urllib.request import urlopen
from urllib.parse import unquote, urlparse
from urllib.error import HTTPError
from zipfile import ZipFile
import tarfile
import shutil

CHUNK_SIZE = 40960
DATA_SOURCE_MAPPING = 'gemma/pytorch/7b-it-quant/2:https%3A%2F%2Fstorage.googleapis.com%2Fkaggle-models-data%2F8749%2F11359%2Fbundle%2Farchive.tar.gz%3FX-Goog-Algorithm%3DGOOG4-RSA-SHA256%26X-Goog-Credential%3Dgcp-kaggle-com%2540kaggle-161607.iam.gserviceaccount.com%252F20240514%252Fauto%252Fstorage%252Fgoog4_request%26X-Goog-Date%3D20240514T072102Z%26X-Goog-Expires%3D259200%26X-Goog-SignedHeaders%3Dhost%26X-Goog-Signature%3D7f2d69b7f0a5dbb8e7a035fdb9afe69e717519e36f832aac5f6db8c798014aa3dffe2d9d7451f1ce245384475df480d348b6ac5e06a0dc76f94bafd48658b911bf2b89afecdeb1ae81a89d4f68ec2aa7a707a63e864158e3cd82a587000589d38f41f57e0d280cc7f3be78099cf90a493d654289096c70f6d9bd593a7e2c22cb602a37ab3a3ff9035fce6c7da553d1594cda9dfd908e6b614bc7473737e319bf3523c74537ce896bdd1c4e5681ba674dfb8932b2969d91ff527528d53174bc41e375b8150df8d4a24b85674827e35fb3c3fae93073cb65bc44dd2955522bbdf662d66b1b08fab4736070e8def2e9a8ed761d79434c107499be8193025ce3f43c'

KAGGLE_INPUT_PATH='/kaggle/input'
KAGGLE_WORKING_PATH='/kaggle/working'
KAGGLE_SYMLINK='kaggle'

!umount /kaggle/input/ 2> /dev/null
shutil.rmtree('/kaggle/input', ignore_errors=True)
os.makedirs(KAGGLE_INPUT_PATH, 0o777, exist_ok=True)
os.makedirs(KAGGLE_WORKING_PATH, 0o777, exist_ok=True)

try:
  os.symlink(KAGGLE_INPUT_PATH, os.path.join("..", 'input'), target_is_directory=True)
except FileExistsError:
  pass
try:
  os.symlink(KAGGLE_WORKING_PATH, os.path.join("..", 'working'), target_is_directory=True)
except FileExistsError:
  pass

for data_source_mapping in DATA_SOURCE_MAPPING.split(','):
    directory, download_url_encoded = data_source_mapping.split(':')
    download_url = unquote(download_url_encoded)
    filename = urlparse(download_url).path
    destination_path = os.path.join(KAGGLE_INPUT_PATH, directory)
    try:
        with urlopen(download_url) as fileres, NamedTemporaryFile() as tfile:
            total_length = fileres.headers['content-length']
            print(f'Downloading {directory}, {total_length} bytes compressed')
            dl = 0
            data = fileres.read(CHUNK_SIZE)
            while len(data) > 0:
                dl += len(data)
                tfile.write(data)
                done = int(50 * dl / int(total_length))
                sys.stdout.write(f"\r[{'=' * done}{' ' * (50-done)}] {dl} bytes downloaded")
                sys.stdout.flush()
                data = fileres.read(CHUNK_SIZE)
            if filename.endswith('.zip'):
              with ZipFile(tfile) as zfile:
                zfile.extractall(destination_path)
            else:
              with tarfile.open(tfile.name) as tarfile:
                tarfile.extractall(destination_path)
            print(f'\nDownloaded and uncompressed: {directory}')
    except HTTPError as e:
        print(f'Failed to load (likely expired) {download_url} to path {destination_path}')
        continue
    except OSError as e:
        print(f'Failed to load {download_url} to path {destination_path}')
        continue

print('Data source import complete.')

# Setup the environment
!pip install -q -U immutabledict sentencepiece
!git clone https://github.com/google/gemma_pytorch.git
!mkdir /kaggle/working/gemma/
!mv /kaggle/working/gemma_pytorch/gemma/* /kaggle/working/gemma/

import sys
sys.path.append("/kaggle/working/gemma_pytorch/")
from gemma.config import GemmaConfig, get_config_for_7b, get_config_for_2b
from gemma.model import GemmaForCausalLM
from gemma.tokenizer import Tokenizer
import contextlib
import os
import torch

# Load the model
VARIANT = "7b-it-quant"
MACHINE_TYPE = "cuda"
weights_dir = '/kaggle/input/gemma/pytorch/7b-it-quant/2'

@contextlib.contextmanager
def _set_default_tensor_type(dtype: torch.dtype):
  """Sets the default torch dtype to the given dtype."""
  torch.set_default_dtype(dtype)
  yield
  torch.set_default_dtype(torch.float)

# Model Config.
model_config = get_config_for_2b() if "2b" in VARIANT else get_config_for_7b()
model_config.tokenizer = os.path.join(weights_dir, "tokenizer.model")
model_config.quant = "quant" in VARIANT

# Model.
device = torch.device(MACHINE_TYPE)
with _set_default_tensor_type(model_config.get_dtype()):
  model = GemmaForCausalLM(model_config)
  ckpt_path = os.path.join(weights_dir, f'gemma-{VARIANT}.ckpt')
  model.load_weights(ckpt_path)
  model = model.to(device).eval()


# Use the model

# USER_CHAT_TEMPLATE = "<start_of_turn>user\n{prompt}<end_of_turn>\n"
# MODEL_CHAT_TEMPLATE = "<start_of_turn>model\n{prompt}<end_of_turn>\n"

# prompt = (
#     USER_CHAT_TEMPLATE.format(
#         prompt="What is a good place for travel in the US?"
#     )
#     + MODEL_CHAT_TEMPLATE.format(prompt="California.")
#     + USER_CHAT_TEMPLATE.format(prompt="What can I do in California?")
#     + "<start_of_turn>model\n"
# )

# model.generate(
#     USER_CHAT_TEMPLATE.format(prompt=prompt),
#     device=device,
#     output_len=100,
# )

# prompt_template = """
# [CONTEXT]
# The commands entered by the user are either basic commands used in Ubuntu or execution commands for separate utilities or applications such as git or docker.
# However, the user's command was entered incorrectly, resulting in an error.
# Modify the command according to [STEP] below.

# [STEP]
# check: Checks whether the user's command is truly incorrect. If there is a mistake, print true. Otherwise, print false.
# incollect_point: If something is absolutely wrong, please let us know by wrapping the incorrect location in **. (Example: aptget install docker -> *aptget* install docker). If not correct, print null
# incollect_reason: Please tell us why the incorrect location found in 2. is incorrect. (Example: aptget install docker -> *aptget* install docker -> “apt-get”, not “aptget”)
# fixed_command: Please correct the correct command according to the incorrect reason stated in 3. If the result of the check in step 1 is correct, print the user's input as is.

# [OUTPUT]
# format : json
# {{
# “incollected_command” : {command}
# “check” : {{{{true | false}}}},
# “incollect_point” : {{{{incollect_point | null}}}},
# “incollect_reason” : {{{{incollect_reason | null}}}},
# “fixed_command” : {{{{fixed_command}}}}
# }}

# [INSTRUCTION]
# Modify the incorrect command below to the correct command as described in [COTEXT] and output it in the format specified in [OUTPUT].
# ---
# user : aptget install docker
# assiatant : {{
# "incollected_command" : "aptget install docker",
# “check” : true,
# “incollect_point” : “*aptget* install docker”,
# “incollect_reason” : “aptget”이 아니라 “apt-get”입니다" ,
# “fixed_command” : "apt-get install docker"
# }}
# ---
# user : git comit -m “message”
# assiatant : {{
# "incollected_command" : "git comit -m \\“message\\”",
# “check” : true,
# “incollect_point” : "git *comit* -m \\“message\\”",
# “incollect_reason” : “comit”이 아니라 “commit”입니다" ,
# “fixed_command” : "git commit –m “message”"
# }}
# ---
# user : {command}
# assistant :
# """

# prompt = prompt_template.format(command="dokcer run -it -p 80:80 nginx")

prompt_template = """
[CONTEXT]
You are an academic consultant for high school students. The user will provide information on their current grades, desired career, and areas they wish to learn more about. Based on the user's information and goals, provide methods for improving grades, target colleges or workplaces, effective study methods, study scheduling, recommended curriculum, and specific advice on career paths.

[USER INFORMATION]
{{"user's current grades, desired career, areas they want to learn about"}}
"Current grades": {grades}
"Desired career": {career}
"Areas of interest": {areas}


[INSTRUCTION]
Based on the [CONTEXT] and [USER INFORMATION], provide the following in the format specified in [OUTPUT]:
- The grades the user needs to achieve
- The target college or workplace
- Recommended methods of study
- Efficient study scheduling
- Suitable curriculum
- Career path advice
---
user : "Current grades", "Desired career", "Areas of interest"
assistant : {{
"Target college/workplace":
"Recommended study methods":
"Efficient study scheduling":
"Suitable curriculum":
"Career path advice":
}}
---
"""

prompt = prompt_template.format(grades="B",
                               career = "공무원",
                               areas = "법")

# Generate sample
model.generate(
    prompt,
    device=device,
    output_len=500,
)

'''
'[OUTPUT]\n\n**Example Output:**\n---\nuser : "Current grades", "Desired career", "Areas of interest"\n
assistant : {\n
"Target college/workplace": "University of California, Berkeley",\n
"Recommended study methods": ["Lecture notes review", "Flashcards", "Active reading"],\n
"Efficient study scheduling": ["Create a weekly schedule", "Stick to the schedule", "Break down tasks into smaller steps"],\n
"Suitable curriculum": ["Political Science", "English", "Math", "Law", "Economics"],\n
"Career path advice": ["Gain experience in government roles", "Volunteer for political campaigns", "Internships in government agencies"]\n}\n
---\n```\n\n**Please provide the requested information below:**\n\n**User\'s current grades:**\n**Desired career:**\n**Areas of interest:**\n\n
**Once you provide the information above, I will be able to generate the output.**'
'''